{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import required libraries and load preprocessed EEG data\n",
    "----------------------------------------------------------------\n",
    "In this step, we import all necessary Python packages for EEG data analysis and machine learning.\n",
    "We then load the preprocessed EEG data from the ADHD and Control groups.\n",
    "The data is organized into a DataFrame with subject ID, class label (ADHD or Control),\n",
    "and EEG channel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to preprocessed data folders\n",
    "data_root = 'ADHD_data'\n",
    "adhd_folders = [\n",
    "    os.path.join(data_root, 'ADHD_part1_preprocessed'),\n",
    "    os.path.join(data_root, 'ADHD_part2_preprocessed')\n",
    "]\n",
    "control_folders = [\n",
    "    os.path.join(data_root, 'Control_part1_preprocessed'),\n",
    "    os.path.join(data_root, 'Control_part2_preprocessed')\n",
    "]\n",
    "\n",
    "# Function to load all preprocessed .mat files and create a DataFrame\n",
    "def load_preprocessed_data(folders, class_label):\n",
    "    all_data = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        mat_files = glob(os.path.join(folder, '*_preprocessed.mat'))\n",
    "        \n",
    "        for mat_file in mat_files:\n",
    "            # Extract subject ID from filename\n",
    "            subject_id = os.path.basename(mat_file).split('_')[0]\n",
    "            \n",
    "            # Load .mat file\n",
    "            data = loadmat(mat_file)\n",
    "            eeg_data = data['preprocessedData']  # Shape: (channels, time_points)\n",
    "            fs = float(data['fs'][0, 0])  # Sampling frequency\n",
    "            \n",
    "            # Create a dictionary with subject info and EEG data\n",
    "            subject_dict = {\n",
    "                'ID': subject_id,\n",
    "                'Class': class_label,\n",
    "                'EEG_Data': eeg_data,\n",
    "                'fs': fs\n",
    "            }\n",
    "            \n",
    "            all_data.append(subject_dict)\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ADHD data...\n",
      "Loaded 61 ADHD subjects\n",
      "Loading Control data...\n",
      "Loaded 60 Control subjects\n",
      "Total subjects: 121\n",
      "Data shape: (121, 4)\n",
      "     ID Class\n",
      "0  v10p  ADHD\n",
      "1  v12p  ADHD\n",
      "2  v14p  ADHD\n",
      "3  v15p  ADHD\n",
      "4  v173  ADHD\n",
      "Sampling frequency: 128.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load ADHD and Control data\n",
    "print(\"Loading ADHD data...\")\n",
    "adhd_data = load_preprocessed_data(adhd_folders, 'ADHD')\n",
    "print(f\"Loaded {len(adhd_data)} ADHD subjects\")\n",
    "\n",
    "print(\"Loading Control data...\")\n",
    "control_data = load_preprocessed_data(control_folders, 'Control')\n",
    "print(f\"Loaded {len(control_data)} Control subjects\")\n",
    "\n",
    "# Combine all data\n",
    "all_subjects = adhd_data + control_data\n",
    "print(f\"Total subjects: {len(all_subjects)}\")\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame(all_subjects)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(data[['ID', 'Class']].head())\n",
    "\n",
    "# Check sampling rate\n",
    "sfreq = data['fs'].iloc[0]\n",
    "print(f\"Sampling frequency: {sfreq} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define frequency bands and create a function to calculate band power\n",
    "---------------------------------------------------------------------------\n",
    "Here we define common EEG frequency bands (delta, theta, alpha, beta) and create\n",
    "a function to calculate the power in each frequency band for a given EEG signal.\n",
    "The function applies a bandpass filter to the signal and then calculates the\n",
    "mean of the squared signal as a measure of power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frequency bands\n",
    "freq_bands = {\n",
    "    'delta': (0.5, 4),\n",
    "    'theta': (4, 8),\n",
    "    'alpha': (8, 13),\n",
    "    'beta': (13, 30)\n",
    "}\n",
    "\n",
    "def calculate_band_power(data, sf, band):\n",
    "    \"\"\"\n",
    "    Calculate the power of a specific frequency band in an EEG signal.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : 1D array\n",
    "        EEG signal time series\n",
    "    sf : float\n",
    "        Sampling frequency in Hz\n",
    "    band : tuple\n",
    "        Frequency band range (low_freq, high_freq) in Hz\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Power in the specified frequency band\n",
    "    \"\"\"\n",
    "    # Get band frequencies\n",
    "    low_freq, high_freq = band\n",
    "    \n",
    "    # Apply bandpass filter using MNE\n",
    "    filtered_data = mne.filter.filter_data(\n",
    "        data.astype(np.float64),  # Convert to float64 for better precision\n",
    "        sfreq=sf,\n",
    "        l_freq=low_freq,\n",
    "        h_freq=high_freq,\n",
    "        method='fir',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Calculate power (mean of squared signal)\n",
    "    power = np.mean(filtered_data ** 2)\n",
    "    \n",
    "    return power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Extract spectral power features for each subject\n",
    "--------------------------------------------------------\n",
    "For each subject, we extract the EEG data for all channels and calculate\n",
    "the power in each frequency band for each channel. We also calculate the\n",
    "Theta/Beta ratio for frontal channels, which is a common biomarker for ADHD.\n",
    "All these features are collected into a feature matrix X_band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed subject v10p (ADHD): 77 features\n",
      "Processed subject v12p (ADHD): 77 features\n",
      "Processed subject v14p (ADHD): 77 features\n",
      "Processed subject v15p (ADHD): 77 features\n",
      "Processed subject v173 (ADHD): 77 features\n",
      "Processed subject v18p (ADHD): 77 features\n",
      "Processed subject v19p (ADHD): 77 features\n",
      "Processed subject v1p (ADHD): 77 features\n",
      "Processed subject v20p (ADHD): 77 features\n",
      "Processed subject v21p (ADHD): 77 features\n",
      "Processed subject v22p (ADHD): 77 features\n",
      "Processed subject v24p (ADHD): 77 features\n",
      "Processed subject v25p (ADHD): 77 features\n",
      "Processed subject v27p (ADHD): 77 features\n",
      "Processed subject v28p (ADHD): 77 features\n",
      "Processed subject v29p (ADHD): 77 features\n",
      "Processed subject v30p (ADHD): 77 features\n",
      "Processed subject v31p (ADHD): 77 features\n",
      "Processed subject v32p (ADHD): 77 features\n",
      "Processed subject v33p (ADHD): 77 features\n",
      "Processed subject v34p (ADHD): 77 features\n",
      "Processed subject v35p (ADHD): 77 features\n",
      "Processed subject v36p (ADHD): 77 features\n",
      "Processed subject v37p (ADHD): 77 features\n",
      "Processed subject v38p (ADHD): 77 features\n",
      "Processed subject v39p (ADHD): 77 features\n",
      "Processed subject v3p (ADHD): 77 features\n",
      "Processed subject v40p (ADHD): 77 features\n",
      "Processed subject v6p (ADHD): 77 features\n",
      "Processed subject v8p (ADHD): 77 features\n",
      "Processed subject v177 (ADHD): 77 features\n",
      "Processed subject v179 (ADHD): 77 features\n",
      "Processed subject v181 (ADHD): 77 features\n",
      "Processed subject v183 (ADHD): 77 features\n",
      "Processed subject v190 (ADHD): 77 features\n",
      "Processed subject v196 (ADHD): 77 features\n",
      "Processed subject v198 (ADHD): 77 features\n",
      "Processed subject v200 (ADHD): 77 features\n",
      "Processed subject v204 (ADHD): 77 features\n",
      "Processed subject v206 (ADHD): 77 features\n",
      "Processed subject v209 (ADHD): 77 features\n",
      "Processed subject v213 (ADHD): 77 features\n",
      "Processed subject v215 (ADHD): 77 features\n",
      "Processed subject v219 (ADHD): 77 features\n",
      "Processed subject v227 (ADHD): 77 features\n",
      "Processed subject v231 (ADHD): 77 features\n",
      "Processed subject v234 (ADHD): 77 features\n",
      "Processed subject v236 (ADHD): 77 features\n",
      "Processed subject v238 (ADHD): 77 features\n",
      "Processed subject v244 (ADHD): 77 features\n",
      "Processed subject v246 (ADHD): 77 features\n",
      "Processed subject v250 (ADHD): 77 features\n",
      "Processed subject v254 (ADHD): 77 features\n",
      "Processed subject v263 (ADHD): 77 features\n",
      "Processed subject v265 (ADHD): 77 features\n",
      "Processed subject v270 (ADHD): 77 features\n",
      "Processed subject v274 (ADHD): 77 features\n",
      "Processed subject v279 (ADHD): 77 features\n",
      "Processed subject v284 (ADHD): 77 features\n",
      "Processed subject v286 (ADHD): 77 features\n",
      "Processed subject v288 (ADHD): 77 features\n",
      "Processed subject v107 (Control): 77 features\n",
      "Processed subject v108 (Control): 77 features\n",
      "Processed subject v109 (Control): 77 features\n",
      "Processed subject v110 (Control): 77 features\n",
      "Processed subject v111 (Control): 77 features\n",
      "Processed subject v112 (Control): 77 features\n",
      "Processed subject v113 (Control): 77 features\n",
      "Processed subject v114 (Control): 77 features\n",
      "Processed subject v115 (Control): 77 features\n",
      "Processed subject v116 (Control): 77 features\n",
      "Processed subject v41p (Control): 77 features\n",
      "Processed subject v42p (Control): 77 features\n",
      "Processed subject v43p (Control): 77 features\n",
      "Processed subject v44p (Control): 77 features\n",
      "Processed subject v45p (Control): 77 features\n",
      "Processed subject v46p (Control): 77 features\n",
      "Processed subject v47p (Control): 77 features\n",
      "Processed subject v48p (Control): 77 features\n",
      "Processed subject v49p (Control): 77 features\n",
      "Processed subject v50p (Control): 77 features\n",
      "Processed subject v51p (Control): 77 features\n",
      "Processed subject v52p (Control): 77 features\n",
      "Processed subject v53p (Control): 77 features\n",
      "Processed subject v54p (Control): 77 features\n",
      "Processed subject v55p (Control): 77 features\n",
      "Processed subject v56p (Control): 77 features\n",
      "Processed subject v57p (Control): 77 features\n",
      "Processed subject v58p (Control): 77 features\n",
      "Processed subject v59p (Control): 77 features\n",
      "Processed subject v60p (Control): 77 features\n",
      "Processed subject v117 (Control): 77 features\n",
      "Processed subject v118 (Control): 77 features\n",
      "Processed subject v120 (Control): 77 features\n",
      "Processed subject v121 (Control): 77 features\n",
      "Processed subject v123 (Control): 77 features\n",
      "Processed subject v125 (Control): 77 features\n",
      "Processed subject v127 (Control): 77 features\n",
      "Processed subject v129 (Control): 77 features\n",
      "Processed subject v131 (Control): 77 features\n",
      "Processed subject v133 (Control): 77 features\n",
      "Processed subject v134 (Control): 77 features\n",
      "Processed subject v138 (Control): 77 features\n",
      "Processed subject v140 (Control): 77 features\n",
      "Processed subject v143 (Control): 77 features\n",
      "Processed subject v147 (Control): 77 features\n",
      "Processed subject v149 (Control): 77 features\n",
      "Processed subject v151 (Control): 77 features\n",
      "Processed subject v297 (Control): 77 features\n",
      "Processed subject v298 (Control): 77 features\n",
      "Processed subject v299 (Control): 77 features\n",
      "Processed subject v300 (Control): 77 features\n",
      "Processed subject v302 (Control): 77 features\n",
      "Processed subject v303 (Control): 77 features\n",
      "Processed subject v304 (Control): 77 features\n",
      "Processed subject v305 (Control): 77 features\n",
      "Processed subject v306 (Control): 77 features\n",
      "Processed subject v307 (Control): 77 features\n",
      "Processed subject v308 (Control): 77 features\n",
      "Processed subject v309 (Control): 77 features\n",
      "Processed subject v310 (Control): 77 features\n"
     ]
    }
   ],
   "source": [
    "# Define frontal channels for Theta/Beta ratio\n",
    "frontal_channels = [0, 1, 2, 3, 4, 5, 16]  # Assuming Fp1, Fp2, F3, F4, F7, F8, Fz\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "band_powers = []\n",
    "subject_ids = []\n",
    "labels = []\n",
    "\n",
    "# Process each subject\n",
    "for idx, subject in data.iterrows():\n",
    "    # Get subject info\n",
    "    subject_id = subject['ID']\n",
    "    subject_class = subject['Class']\n",
    "    eeg_data = subject['EEG_Data']  # Shape: (channels, time_points)\n",
    "    sf = subject['fs']\n",
    "    \n",
    "    # Initialize feature vector for this subject\n",
    "    subject_features = []\n",
    "    \n",
    "    # Calculate band power for each channel and frequency band\n",
    "    num_channels = eeg_data.shape[0]\n",
    "    \n",
    "    for ch in range(num_channels):\n",
    "        channel_data = eeg_data[ch, :]\n",
    "        \n",
    "        # Skip channels with NaN values\n",
    "        if np.isnan(channel_data).any():\n",
    "            # Add zeros for this channel's features\n",
    "            subject_features.extend([0, 0, 0, 0])\n",
    "            continue\n",
    "        \n",
    "        # Calculate power for each frequency band\n",
    "        for band_name, band_range in freq_bands.items():\n",
    "            power = calculate_band_power(channel_data, sf, band_range)\n",
    "            subject_features.append(power)\n",
    "    \n",
    "    # Calculate Theta/Beta ratio for frontal channels\n",
    "    theta_powers = []\n",
    "    beta_powers = []\n",
    "    \n",
    "    for ch_idx in frontal_channels:\n",
    "        if ch_idx < num_channels:  # Make sure channel exists\n",
    "            channel_data = eeg_data[ch_idx, :]\n",
    "            \n",
    "            # Skip channels with NaN values\n",
    "            if np.isnan(channel_data).any():\n",
    "                continue\n",
    "            \n",
    "            theta_power = calculate_band_power(channel_data, sf, freq_bands['theta'])\n",
    "            beta_power = calculate_band_power(channel_data, sf, freq_bands['beta'])\n",
    "            \n",
    "            if beta_power > 0:  # Avoid division by zero\n",
    "                theta_powers.append(theta_power)\n",
    "                beta_powers.append(beta_power)\n",
    "    \n",
    "    # Calculate average Theta/Beta ratio across frontal channels\n",
    "    if len(beta_powers) > 0:\n",
    "        avg_theta = np.mean(theta_powers)\n",
    "        avg_beta = np.mean(beta_powers)\n",
    "        theta_beta_ratio = avg_theta / avg_beta\n",
    "    else:\n",
    "        theta_beta_ratio = 0\n",
    "    \n",
    "    # Add Theta/Beta ratio to features\n",
    "    subject_features.append(theta_beta_ratio)\n",
    "    \n",
    "    # Add features to the list\n",
    "    band_powers.append(subject_features)\n",
    "    subject_ids.append(subject_id)\n",
    "    labels.append(1 if subject_class == 'ADHD' else 0)\n",
    "    \n",
    "    print(f\"Processed subject {subject_id} ({subject_class}): {len(subject_features)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (121, 77)\n",
      "Label vector shape: (121,)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "X_band = np.array(band_powers)\n",
    "y_band = np.array(labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_band.shape}\")\n",
    "print(f\"Label vector shape: {y_band.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Prepare labels and ensure consistent ordering\n",
    "----------------------------------------------------\n",
    "We ensure that the labels (y_band) are correctly aligned with the feature matrix (X_band)\n",
    "by maintaining the same order of subject IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of subjects and their labels:\n",
      "     ID  Class\n",
      "0  v10p      1\n",
      "1  v12p      1\n",
      "2  v14p      1\n",
      "3  v15p      1\n",
      "4  v173      1\n",
      "\n",
      "Class distribution:\n",
      "ADHD (1): 61\n",
      "Control (0): 60\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to verify alignment\n",
    "feature_df = pd.DataFrame({\n",
    "    'ID': subject_ids,\n",
    "    'Class': y_band\n",
    "})\n",
    "\n",
    "print(\"Sample of subjects and their labels:\")\n",
    "print(feature_df.head())\n",
    "\n",
    "# Check class balance\n",
    "class_counts = feature_df['Class'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"ADHD (1): {class_counts.get(1, 0)}\")\n",
    "print(f\"Control (0): {class_counts.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 84 samples\n",
      "Testing set: 37 samples\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_band, y_band, test_size=0.3, random_state=42, stratify=y_band\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with StandardScaler and SVM\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(probability=True))\n",
    "])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "    'svm__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with GridSearchCV...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Training SVM with GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the performance of the best model on the test set\n",
    "and analyze the results using accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "{'svm__C': 10, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}\n",
      "\n",
      "Test accuracy: 0.4595\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 7 11]\n",
      " [ 9 10]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Control       0.44      0.39      0.41        18\n",
      "        ADHD       0.48      0.53      0.50        19\n",
      "\n",
      "    accuracy                           0.46        37\n",
      "   macro avg       0.46      0.46      0.46        37\n",
      "weighted avg       0.46      0.46      0.46        37\n",
      "\n",
      "\n",
      "Sensitivity (True Positive Rate): 0.5263\n",
      "Specificity (True Negative Rate): 0.3889\n"
     ]
    }
   ],
   "source": [
    "# Print best parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_test, y_pred, target_names=['Control', 'ADHD'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Calculate and print additional metrics\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"\\nSensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "\n",
    "# Analyze feature importance (for linear kernel)\n",
    "if 'linear' in grid_search.best_params_['svm__kernel']:\n",
    "    # Get feature importance from SVM coefficients\n",
    "    feature_importance = np.abs(best_model.named_steps['svm'].coef_[0])\n",
    "    \n",
    "    # Get top 10 features\n",
    "    top_indices = np.argsort(feature_importance)[-10:]\n",
    "    top_importance = feature_importance[top_indices]\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"Feature {idx}: {top_importance[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
